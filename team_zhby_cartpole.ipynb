{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bohyanggg/SC3000_Artificial_Intelligence/blob/main/team_zhby_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial and Sample Code for Balancing a Pole on a Cart"
      ],
      "metadata": {
        "id": "ZauhjPSfX7pI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies:"
      ],
      "metadata": {
        "id": "UBiYOoesYMvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PbgnVwZmX5uW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cefb531-96b7-4d84-c4c2-37243278d6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed pygame-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (6.1.0)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (2.25.1)\n",
            "Collecting pytest==7.0.1\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (3.7.1)\n",
            "Collecting lz4>=3.1.0\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco-py<2.2,>=2.1\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco==2.2.0\n",
            "  Downloading mujoco-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (2.1.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.9/dist-packages (from gym[all]) (4.7.0.72)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.9/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mujoco==2.2.0->gym[all]) (1.4.0)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.7-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.7/207.7 KB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py>=1.8.2\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest==7.0.1->gym[all]) (22.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytest==7.0.1->gym[all]) (23.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest==7.0.1->gym[all]) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest==7.0.1->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.5->gym[all]) (5.12.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio>=2.14.1->gym[all]) (8.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[all]) (3.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (4.39.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (0.29.33)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.9/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.15.1)\n",
            "Collecting fasteners~=0.15\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, glfw, box2d-py, py, mujoco, lz4, fasteners, pytest, mujoco-py, ale-py\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.2.2\n",
            "    Uninstalling pytest-7.2.2:\n",
            "      Successfully uninstalled pytest-7.2.2\n",
            "Successfully installed ale-py-0.7.5 box2d-py-2.3.5 fasteners-0.18 glfw-2.5.7 lz4-4.3.2 mujoco-2.2.0 mujoco-py-2.1.2.14 py-1.11.0 pytest-7.0.1 swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[classic_control]\n",
        "!pip install gym[all]\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dependencies and define helper functions"
      ],
      "metadata": {
        "id": "RwKbYeTgbaTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "metadata": {
        "id": "j6KpgCLGYWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial: Loading CartPole environment"
      ],
      "metadata": {
        "id": "ehbqP9CXbmo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "Go12dH4qbwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."
      ],
      "metadata": {
        "id": "9XZ9g3xrcAXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(env.action_space)"
      ],
      "metadata": {
        "id": "ytxvVmLdcRyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
      ],
      "metadata": {
        "id": "pVXGWi_Ncfg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(env.observation_space)"
      ],
      "metadata": {
        "id": "DyqHr9I5cdkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
      ],
      "metadata": {
        "id": "HFOdaU2Gdyg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observation = env.reset()\n",
        "# print(\"Initial observations:\", observation)"
      ],
      "metadata": {
        "id": "VMr6qAqxdOsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."
      ],
      "metadata": {
        "id": "qnG2QdfbeZrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observation, reward, done, info = env.step(0)\n",
        "# print(\"New observations after choosing action 0:\", observation)\n",
        "# print(\"Reward for this step:\", reward)\n",
        "# print(\"Is this round done?\", done)"
      ],
      "metadata": {
        "id": "MmfMDvyYdWGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
      ],
      "metadata": {
        "id": "tj0zCh59fhBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observation = env.reset()\n",
        "# cumulative_reward = 0\n",
        "# done = False\n",
        "# while not done:\n",
        "#     observation, reward, done, info = env.step(0)\n",
        "#     cumulative_reward += reward\n",
        "# print(\"Cumulative reward for this round:\", cumulative_reward)"
      ],
      "metadata": {
        "id": "AVucQVRwf6Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Development of an RL agent"
      ],
      "metadata": {
        "id": "2oIzK9SzhlWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of a naive agent is given below, which randomly chooses an action regardless of the observation:"
      ],
      "metadata": {
        "id": "Cc6_e5c_huiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_policy_agent(observation):\n",
        "    return random.randint(0, 1)"
      ],
      "metadata": {
        "id": "Hk-M4QEfh6l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# define hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.01\n",
        "\n",
        "# create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# initialize Q-table\n",
        "n_states = 10**env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "# define function to discretize observation space\n",
        "def discretize(observation):\n",
        "    # create bounds for each feature of observation\n",
        "    bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "    # map each feature to a discrete value\n",
        "    discrete_observation = [int(np.digitize(obs, b)) for obs, b in zip(observation, bounds)]\n",
        "    # convert discrete observation to single integer\n",
        "    state = sum([val*(10**i) for i, val in enumerate(discrete_observation)])\n",
        "    return state\n",
        "\n",
        "# define function to choose action based on epsilon-greedy policy\n",
        "def choose_action(state, q_table, epsilon):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = env.action_space.sample()  # explore\n",
        "    else:\n",
        "        action = np.argmax(q_table[state])  # exploit\n",
        "    return action\n",
        "\n",
        "# define function to train Q-learning agent\n",
        "def train_q_learning_agent(env):\n",
        "    episode_rewards = []\n",
        "    for i_episode in range(10000):\n",
        "        state = discretize(env.reset())  # discretize initial observation to get initial state\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = choose_action(state, q_table, epsilon)\n",
        "            next_observation, reward, done, info = env.step(action)\n",
        "            next_state = discretize(next_observation)\n",
        "            episode_reward += reward\n",
        "            # update Q-table\n",
        "            q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
        "            state = next_state\n",
        "        episode_rewards.append(episode_reward)\n",
        "        # check if task is solved (total reward for episode is greater than 195)\n",
        "        if episode_reward > 195:\n",
        "            break\n",
        "    return q_table, episode_rewards\n",
        "\n",
        "# train agent\n",
        "q_table, episode_rewards = train_q_learning_agent(env)\n",
        "\n",
        "# plot episode rewards\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Q-learning Performance')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VVOnGyf7KqBg",
        "outputId": "d9fc3786-ea14-4163-d556-2834718895ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8026ae6ae69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# train agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_q_learning_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# save video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-8026ae6ae69a>\u001b[0m in \u001b[0;36mtrain_q_learning_agent\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# record frame to video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mvideo_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# check if task is solved (total reward for episode is greater than 195)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video_frames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Task 1, we can show the observation and chosen action below:"
      ],
      "metadata": {
        "id": "RAi7KKwNiegR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observation = env.reset()\n",
        "# action = rand_policy_agent(observation)\n",
        "# print(\"Observation:\", observation)\n",
        "# print(\"Chosen action:\", action)"
      ],
      "metadata": {
        "id": "ae2ia-vUiNKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Demonstrate the effectiveness of the RL agent"
      ],
      "metadata": {
        "id": "-XtIQ0Rti1gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, use the agent developed in Task 1 to play the game for 100 episodes (refer to tutorial for how to play a round), record the cumulative reward for each round, and plot the reward for each round. A sample plotting code is given below. Note that you must include code to play for 100 episodes and use the code to obtain round_results for plotting. DO NOT record the round results in advance and paste the results to the notebook."
      ],
      "metadata": {
        "id": "djBEShf0kGI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# episode_results = np.random.randint(150, 2500, size=100)\n",
        "# plt.plot(episode_results)\n",
        "# plt.title('Cumulative reward for each episode')\n",
        "# plt.ylabel('Cumulative reward')\n",
        "# plt.xlabel('episode')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "RZrCKywQi6CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the average reward over the 100 episodes."
      ],
      "metadata": {
        "id": "XndSYH7wlvn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Average cumulative reward:\", episode_results.mean())\n",
        "# print(\"Is my agent good enough?\", episode_results.mean() > 195)"
      ],
      "metadata": {
        "id": "pOiOp9OYlo5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Render one episode played by the agent"
      ],
      "metadata": {
        "id": "Yg0DCT38lFA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plug your agent to the code below to obtain rendered result."
      ],
      "metadata": {
        "id": "vx1awMr9lc_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "# observation = env.reset()\n",
        "# while True:\n",
        "#     env.render()\n",
        "#     #your agent goes here\n",
        "#     action = rand_policy_agent(observation)\n",
        "#     #action = train_q_learning_agent(env)\n",
        "#     observation, reward, done, info = env.step(action) \n",
        "#     if done: \n",
        "#       break;    \n",
        "# env.close()\n",
        "# show_video()\n",
        "\n",
        "# render one episode played by the agent using RecordVideo function\n",
        "video_env = RecordVideo(env, \"./video.mp4\")\n",
        "observation = video_env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    video_env.render()\n",
        "    action = np.argmax(q_table[discretize(observation)])\n",
        "    observation, reward, done, info = video_env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "# close environment and show video\n",
        "video_env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "LYyavfbIa47D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "259cd359-29d9-40e0-c57d-5067e56bcd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}